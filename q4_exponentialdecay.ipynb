{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zyythn/Assignment-MV/blob/main/q4_exponentialdecay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbTvGFjap6xj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MnBReGEQ0_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "652c94ec-30c6-46fc-f5fa-f17809494317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JwgzXfVqkKI"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.Resize((224,224)),\n",
        "     transforms.ToTensor(), # convert to 4d-tensor\n",
        "     transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])]\n",
        ")\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/MV/tom and jerry/train'\n",
        "test_dir = '/content/drive/MyDrive/MV/tom and jerry/test'\n",
        "\n",
        "train_data = datasets.ImageFolder(root=train_dir,\n",
        "                                  transform=transform)\n",
        "test_data = datasets.ImageFolder(root=test_dir,\n",
        "                                  transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Halz5DmYqkRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8357604-69a7-4f6e-e9e5-bd08bbc1ada6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jerry', 'no tom jerry', 'tom', 'tom jerry']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cerOS3SNqkW1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_data,batch_size=4, shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(test_data,batch_size=4, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMamgLGaTqt2"
      },
      "outputs": [],
      "source": [
        "# Define our model\n",
        "class CNNmodel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNNmodel,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5) #36x36x6\n",
        "    self.maxpool1 = nn.MaxPool2d(kernel_size=2,stride=2) #18x18x6\n",
        "    self.conv2 = nn.Conv2d(6,16,5) #14x14x16\n",
        "    self.maxpool2 = nn.MaxPool2d(kernel_size=2,stride=2) #7x7x16\n",
        "    self.conv3 = nn.Conv2d(16, 20, 3) #5x5x20\n",
        "    self.fc1 = nn.Linear(52020,20*5*5)\n",
        "    self.fc2 = nn.Linear(20*5*5,4)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.batchnorm1 = nn.BatchNorm2d(6)\n",
        "    self.batchnorm2 = nn.BatchNorm2d(16)\n",
        "    self.dropout = nn.Dropout(0.4) #drop 40% of neurons during training\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.batchnorm1(x)\n",
        "    x = self.maxpool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.batchnorm2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.relu(x)\n",
        "    out = self.fc2(x)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKn59nMHUA8c"
      },
      "outputs": [],
      "source": [
        "model = CNNmodel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDPStatMqkiB"
      },
      "outputs": [],
      "source": [
        "# cross-entropy loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn.to('cuda')\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Dtng1xLZttx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "058c6663-fc6a-408b-ca22-1a42b1eb2637"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNmodel(\n",
              "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv2d(16, 20, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=52020, out_features=500, bias=True)\n",
              "  (fc2): Linear(in_features=500, out_features=4, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (batchnorm1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batchnorm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout): Dropout(p=0.4, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tO5YAV5xqknc"
      },
      "outputs": [],
      "source": [
        "# Learning rate scheduler - Exponential Decay\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "#Exponential Learning Rate\n",
        "lr = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m14ytJq7umgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd630a79-5180-479e-b01f-b0bbe7916a90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Accuracy: 48.4690%, \n",
            "\t\tValidation : Accuracy: 34.4073%\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Accuracy: 73.0862%, \n",
            "\t\tValidation : Accuracy: 35.6404%\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Accuracy: 83.9645%, \n",
            "\t\tValidation : Accuracy: 42.7208%\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Accuracy: 91.8211%, \n",
            "\t\tValidation : Accuracy: 38.4248%\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Accuracy: 94.0774%, \n",
            "\t\tValidation : Accuracy: 35.8393%\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Accuracy: 95.2458%, \n",
            "\t\tValidation : Accuracy: 36.6348%\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Accuracy: 96.5351%, \n",
            "\t\tValidation : Accuracy: 39.0215%\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Accuracy: 97.0185%, \n",
            "\t\tValidation : Accuracy: 38.7033%\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Accuracy: 97.5020%, \n",
            "\t\tValidation : Accuracy: 38.4646%\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Accuracy: 97.7438%, \n",
            "\t\tValidation : Accuracy: 37.4702%\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"Epoch: {}/{}\".format(epoch+1, num_epochs))\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  train_loss = 0.0\n",
        "  train_acc = 0.0\n",
        "\n",
        "  valid_loss = 0.0\n",
        "  valid_acc = 0.0\n",
        "\n",
        "  for inputs, labels in train_dataloader:\n",
        "\n",
        "    inputs = inputs.to('cuda')\n",
        "    labels = labels.to('cuda')\n",
        "\n",
        "    # Clean existing gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass - compute outputs on input data using the model\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = loss_fn(outputs, labels)\n",
        "\n",
        "    # Backpropagate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Compute the total loss for the batch and add it to train_loss\n",
        "    train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    # Compute the accuracy\n",
        "    ret, predictions = torch.max(outputs.data, 1)\n",
        "    correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "    # Convert correct_counts to float and then compute the mean\n",
        "    acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "    # Compute total accuracy in the whole batch and add to train_acc\n",
        "    train_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "  # Validation - No gradient tracking needed\n",
        "  with torch.no_grad():\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Validation loop\n",
        "    for inputs, labels in test_dataloader:\n",
        "      inputs = inputs.to('cuda')\n",
        "      labels = labels.to('cuda')\n",
        "\n",
        "      # Forward pass - compute outputs on input data using the model\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      # Compute the total loss for the batch and add it to valid_loss\n",
        "      valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "      # Calculate validation accuracy\n",
        "      ret, predictions = torch.max(outputs.data, 1)\n",
        "      correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "      # Convert correct_counts to float and then compute the mean\n",
        "      acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "      # Compute total accuracy in the whole batch and add to valid_acc\n",
        "      valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "\n",
        "  # Find average training loss and training accuracy\n",
        "  avg_train_loss = train_loss / len(train_dataloader.dataset)\n",
        "  avg_train_acc = train_acc / len(train_dataloader.dataset)\n",
        "\n",
        "  # Find average validation loss and training accuracy\n",
        "  avg_test_loss = valid_loss / len(test_dataloader.dataset)\n",
        "  avg_test_acc = valid_acc / len(test_dataloader.dataset)\n",
        "\n",
        "  # Update the learning rate\n",
        "  scheduler.step()\n",
        "\n",
        "  lr.append(scheduler.get_last_lr())\n",
        "\n",
        "  print(\"Epoch : {:03d}, Training: Accuracy: {:.4f}%, \\n\\t\\tValidation : Accuracy: {:.4f}%\".format(epoch, avg_train_acc * 100, avg_test_acc * 100))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}